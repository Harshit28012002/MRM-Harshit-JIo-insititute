{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3IFpQ3LjuTum65aUKCNPR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harshit28012002/MRM-Harshit-JIo-insititute/blob/main/Logistic%20regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "3mZakQZC0XSR",
        "outputId": "6d72bca2-0345-40f6-c777-651e689855c1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-ea4d5b4c3077>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Encode categorical variables using OneHotEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'first'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mencoded_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcategorical_columns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mencoded_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategorical_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Define file path\n",
        "file_path = \"/content/bank-full.csv\"\n",
        "\n",
        "# Load dataset with correct delimiter (comma-separated values)\n",
        "df = pd.read_csv(file_path, sep=\",\", engine=\"python\")\n",
        "\n",
        "# Remove any leading/trailing spaces from column names\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# Convert target variable 'y' to binary (0 = no, 1 = yes)\n",
        "df['y'] = df['y'].map({'no': 0, 'yes': 1})\n",
        "\n",
        "# Identify categorical columns\n",
        "categorical_columns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n",
        "\n",
        "# Encode categorical variables using OneHotEncoder\n",
        "encoder = OneHotEncoder(drop='first', sparse=False, handle_unknown='ignore')\n",
        "encoded_features = encoder.fit_transform(df[categorical_columns])\n",
        "encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_columns))\n",
        "\n",
        "# Drop original categorical columns and concatenate encoded features\n",
        "df = df.drop(columns=categorical_columns).reset_index(drop=True)\n",
        "df = pd.concat([df, encoded_df], axis=1)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = df.drop(columns=['y'])\n",
        "y = df['y']\n",
        "\n",
        "# Standardize numerical features\n",
        "numerical_columns = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\n",
        "scaler = StandardScaler()\n",
        "X[numerical_columns] = scaler.fit_transform(X[numerical_columns])\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Define pipeline with logistic regression and standardization\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', LogisticRegression(max_iter=1000))\n",
        "])\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'classifier__C': [0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
        "    'classifier__solver': ['lbfgs', 'liblinear']  # Different solvers\n",
        "}\n",
        "\n",
        "# Perform Grid Search with Cross-Validation\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='recall', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model from grid search\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Train and predict using the best model\n",
        "y_pred_best = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "best_accuracy = accuracy_score(y_test, y_pred_best)\n",
        "best_classification_report = classification_report(y_test, y_pred_best)\n",
        "\n",
        "# Display results\n",
        "print(\"\\nBest Model Parameters:\", grid_search.best_params_)\n",
        "print(\"\\nModel Accuracy:\", best_accuracy)\n",
        "print(\"\\nClassification Report:\\n\", best_classification_report)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Define file path\n",
        "file_path = \"/content/bank-full.csv\"\n",
        "\n",
        "# Load dataset with correct delimiter (comma-separated values)\n",
        "df = pd.read_csv(file_path, sep=\",\", engine=\"python\")\n",
        "\n",
        "# Remove any leading/trailing spaces from column names\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# Convert target variable 'y' to binary (0 = no, 1 = yes)\n",
        "df['y'] = df['y'].map({'no': 0, 'yes': 1})\n",
        "\n",
        "# Identify categorical columns\n",
        "categorical_columns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n",
        "\n",
        "# Encode categorical variables using OneHotEncoder (Fix: Use sparse_output=False)\n",
        "encoder = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
        "encoded_features = encoder.fit_transform(df[categorical_columns])\n",
        "encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_columns))\n",
        "\n",
        "# Drop original categorical columns and concatenate encoded features\n",
        "df = df.drop(columns=categorical_columns).reset_index(drop=True)\n",
        "df = pd.concat([df, encoded_df], axis=1)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = df.drop(columns=['y'])\n",
        "y = df['y']\n",
        "\n",
        "# Standardize numerical features\n",
        "numerical_columns = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\n",
        "scaler = StandardScaler()\n",
        "X[numerical_columns] = scaler.fit_transform(X[numerical_columns])\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Define pipeline with logistic regression and standardization\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', LogisticRegression(max_iter=1000))\n",
        "])\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'classifier__C': [0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
        "    'classifier__solver': ['lbfgs', 'liblinear']  # Different solvers\n",
        "}\n",
        "\n",
        "# Perform Grid Search with Cross-Validation\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='recall', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model from grid search\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Train and predict using the best model\n",
        "y_pred_best = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "best_accuracy = accuracy_score(y_test, y_pred_best)\n",
        "best_classification_report = classification_report(y_test, y_pred_best)\n",
        "\n",
        "# Display results\n",
        "print(\"\\nBest Model Parameters:\", grid_search.best_params_)\n",
        "print(\"\\nModel Accuracy:\", best_accuracy)\n",
        "print(\"\\nClassification Report:\\n\", best_classification_report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1S_12tuN1-Tm",
        "outputId": "650fbe80-53ec-4af7-9321-1ecfe47a2aa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best Model Parameters: {'classifier__C': 10, 'classifier__solver': 'lbfgs'}\n",
            "\n",
            "Model Accuracy: 0.901249585314608\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.97      0.95      7985\n",
            "           1       0.64      0.35      0.45      1058\n",
            "\n",
            "    accuracy                           0.90      9043\n",
            "   macro avg       0.78      0.66      0.70      9043\n",
            "weighted avg       0.89      0.90      0.89      9043\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Define file path\n",
        "file_path = \"/content/bank-full.csv\"\n",
        "\n",
        "# Load dataset with correct delimiter (comma-separated values)\n",
        "df = pd.read_csv(file_path, sep=\",\", engine=\"python\")\n",
        "\n",
        "# Remove any leading/trailing spaces from column names\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# Convert target variable 'y' to binary (0 = no, 1 = yes)\n",
        "df['y'] = df['y'].map({'no': 0, 'yes': 1})\n",
        "\n",
        "# Identify categorical columns\n",
        "categorical_columns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n",
        "\n",
        "# Encode categorical variables using OneHotEncoder\n",
        "encoder = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
        "encoded_features = encoder.fit_transform(df[categorical_columns])\n",
        "encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_columns))\n",
        "\n",
        "# Drop original categorical columns and concatenate encoded features\n",
        "df = df.drop(columns=categorical_columns).reset_index(drop=True)\n",
        "df = pd.concat([df, encoded_df], axis=1)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = df.drop(columns=['y'])\n",
        "y = df['y']\n",
        "\n",
        "# Standardize numerical features\n",
        "numerical_columns = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\n",
        "scaler = StandardScaler()\n",
        "X[numerical_columns] = scaler.fit_transform(X[numerical_columns])\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Train logistic regression model without hyperparameter tuning (for interpretation)\n",
        "log_model = LogisticRegression(max_iter=1000)\n",
        "log_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = log_model.predict(X_test)\n",
        "\n",
        "# Evaluate model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "# Extract feature coefficients\n",
        "coefficients = pd.DataFrame({\n",
        "    \"Feature\": X.columns,\n",
        "    \"Coefficient\": log_model.coef_.flatten(),\n",
        "    \"Odds Ratio\": np.exp(log_model.coef_).flatten()  # Convert log-odds to odds ratio\n",
        "})\n",
        "\n",
        "# Display model results\n",
        "print(\"\\nModel Accuracy:\", accuracy)\n",
        "print(\"\\nClassification Report:\\n\", classification_rep)\n",
        "print(\"\\nFeature Importance (Coefficient & Odds Ratio):\\n\", coefficients.sort_values(by=\"Odds Ratio\", ascending=False))\n",
        "\n",
        "# Display coefficients dataframe for analysis\n",
        "import ace_tools as tools\n",
        "tools.display_dataframe_to_user(name=\"Feature Impact Analysis\", dataframe=coefficients)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "X19dcXw61y4N",
        "outputId": "5a055ccc-5d33-4bdd-de63-1caf510ead7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Accuracy: 0.9014707508570164\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.97      0.95      7985\n",
            "           1       0.65      0.35      0.45      1058\n",
            "\n",
            "    accuracy                           0.90      9043\n",
            "   macro avg       0.78      0.66      0.70      9043\n",
            "weighted avg       0.89      0.90      0.89      9043\n",
            "\n",
            "\n",
            "Feature Importance (Coefficient & Odds Ratio):\n",
            "                 Feature  Coefficient  Odds Ratio\n",
            "40     poutcome_success     2.330223   10.280232\n",
            "34            month_mar     1.433456    4.193165\n",
            "3              duration     1.083559    2.955177\n",
            "38            month_sep     0.810951    2.250046\n",
            "37            month_oct     0.808015    2.243450\n",
            "29            month_dec     0.693758    2.001222\n",
            "33            month_jun     0.417923    1.518803\n",
            "21   education_tertiary     0.376500    1.457175\n",
            "14          job_student     0.346911    1.414691\n",
            "11          job_retired     0.318401    1.374928\n",
            "39       poutcome_other     0.270183    1.310205\n",
            "20  education_secondary     0.201503    1.223239\n",
            "22    education_unknown     0.196481    1.217112\n",
            "2                   day     0.084183    1.087827\n",
            "19       marital_single     0.080611    1.083949\n",
            "1               balance     0.047379    1.048519\n",
            "5                 pdays     0.024248    1.024545\n",
            "6              previous     0.024222    1.024518\n",
            "41     poutcome_unknown     0.023368    1.023643\n",
            "0                   age    -0.015464    0.984655\n",
            "23          default_yes    -0.076686    0.926181\n",
            "26    contact_telephone    -0.113075    0.893084\n",
            "10       job_management    -0.144302    0.865627\n",
            "17          job_unknown    -0.147016    0.863280\n",
            "30            month_feb    -0.182235    0.833405\n",
            "15       job_technician    -0.191493    0.825725\n",
            "16       job_unemployed    -0.203453    0.815908\n",
            "18      marital_married    -0.206711    0.813254\n",
            "13         job_services    -0.224114    0.799224\n",
            "4              campaign    -0.266319    0.766195\n",
            "7       job_blue-collar    -0.306562    0.735973\n",
            "8      job_entrepreneur    -0.343963    0.708955\n",
            "12    job_self-employed    -0.361744    0.696460\n",
            "25             loan_yes    -0.433571    0.648190\n",
            "35            month_may    -0.470681    0.624577\n",
            "9         job_housemaid    -0.548968    0.577546\n",
            "24          housing_yes    -0.676916    0.508182\n",
            "28            month_aug    -0.762787    0.466365\n",
            "32            month_jul    -0.865686    0.420763\n",
            "36            month_nov    -0.908849    0.402988\n",
            "31            month_jan    -1.265367    0.282136\n",
            "27      contact_unknown    -1.592285    0.203460\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'ace_tools'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-946c07d09971>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;31m# Display coefficients dataframe for analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mace_tools\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay_dataframe_to_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Feature Impact Analysis\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoefficients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ace_tools'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}